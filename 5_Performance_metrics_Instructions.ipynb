{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_Performance_metrics_Instructions.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdnew/Performance-Metric-without-any-library/blob/main/5_Performance_metrics_Instructions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Ej_bXyQvnV"
      },
      "source": [
        "# Compute performance metrics for the given Y and Y_score without sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CHb6NE7Qvnc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# other than these two you should not import any other packages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbsWXuDaQvnq"
      },
      "source": [
        "<pre>\n",
        "<font color='red'><b>A.</b></font> Compute performance metrics for the given data <strong>5_a.csv</strong>\n",
        "   <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
        "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
        "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
        "\n",
        "<pre>\n",
        "<ol>\n",
        "<li> Compute Confusion Matrix </li>\n",
        "<li> Compute F1 Score </li>\n",
        "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)</li>\n",
        "<li> Compute Accuracy Score </li>\n",
        "</ol>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaFLW7oBQvnt"
      },
      "source": [
        "# write your code here\n",
        "data_a = pd.read_csv(\"/content/5_a.csv\")\n",
        "data_a.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQtNj6VD5AjH"
      },
      "source": [
        "# predict defination\n",
        "def predict(data, y, thresh_hold):\n",
        "  '''\n",
        "  Argument:\n",
        "  data, DataFrame\n",
        "  y = 'y' column in dataframe\n",
        "  thresh_hold = Thresh hold value for predict method\n",
        "\n",
        "  AIM: predict the value as '1' if y value is more than 0.5 else it will be '0'\n",
        "\n",
        "  Output:\n",
        "  will return y_pred list as return type with classified value either '1' or '0'\n",
        "  '''\n",
        "  #initialize y_pred list\n",
        "  y_pred = []\n",
        "  # check label with thresh_hold value (0.5)\n",
        "  for label in data[y]:\n",
        "    if label<thresh_hold:\n",
        "      y_pred.append(0)\n",
        "    else:\n",
        "      y_pred.append(1)\n",
        "  return y_pred\n",
        "\n",
        "# confusion matrix calculation method\n",
        "def calculate_confusion(data, y, y_pred):\n",
        "  '''\n",
        "    Argument:\n",
        "    data = DataFrame \n",
        "    y = 'y' column in dataframe \n",
        "    y_pred = 'y_pred' column in dataframe\n",
        "\n",
        "    AIM: calculate confusion matrix: TP, TN, FN, FP\n",
        "\n",
        "    Output: dictionary with TN, TP, FN, FP as key and value pair\n",
        "  '''\n",
        "\n",
        "  # initialize variables\n",
        "  tp = 0\n",
        "  tn = 0\n",
        "  fn = 0\n",
        "  fp = 0\n",
        "\n",
        "  # confuntion matrix tp, tn, fn, fp calculation logic\n",
        "  for value1, value2 in enumerate(data['y']):\n",
        "    if (data.y_pred[value1]==1) and data.y[value1] == 1:\n",
        "      tp = tp + 1\n",
        "    if (data.y_pred[value1]==0) and data.y[value1] == 0:\n",
        "      tn = tn + 1\n",
        "    if (data.y_pred[value1]==0) and data.y[value1] == 1:\n",
        "      fn = fn + 1\n",
        "    if (data.y_pred[value1]==1) and data.y[value1] == 0:\n",
        "      fp = fp + 1\n",
        "  \n",
        "  return { 'tn': tn, 'tp': tp, 'fn': fn, 'fp': fp}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A03BLmCr7XCK"
      },
      "source": [
        "# Thresh hold value declation\n",
        "thresh_hold = 0.5\n",
        "# predicting the value and storing in dataframe as y_pred column\n",
        "data_a['y_pred'] = predict(data_a, 'proba', thresh_hold)\n",
        "\n",
        "# calculating confunsion matrix\n",
        "confusion_matrix = calculate_confusion(data_a, 'y', 'y_pred')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BBrDuyZ7zi_"
      },
      "source": [
        "# confusion matrix values\n",
        "print(\"the confusion matrix is:\", confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U1pZQGw78lx"
      },
      "source": [
        "# F1 score calculation \n",
        "x = data_a.y.value_counts()\n",
        "P = x[1]\n",
        "\n",
        "# precision calculation = TP / (TP + FP)\n",
        "precision = confusion_matrix['tp']/(confusion_matrix['tp']+confusion_matrix['fp'])\n",
        "# recall calculation = TP / P\n",
        "recall = confusion_matrix['tp']/P\n",
        "\n",
        "# F1 score calculation with formula\n",
        "F1 = 2* precision * recall / (precision + recall)\n",
        "\n",
        "# display result of F1 score\n",
        "print(\"the F1 score is:\", F1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKBZAI808_Zs"
      },
      "source": [
        "# Accuracy calculation with formula\n",
        "Acc = (confusion_matrix['tp']+confusion_matrix['tn'])/data_a.shape[0]\n",
        "print(\"The accuracy is:\",Acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHqhDiwB-BHK"
      },
      "source": [
        "# AUC score calculation defination\n",
        "from tqdm import tqdm_notebook \n",
        "def auc(data):\n",
        "  '''\n",
        "  Argument:\n",
        "  data: DataFrame\n",
        "\n",
        "  AIM: This method to calculate the AUC (Area under the curve)\n",
        "      with confuntion matrix, it will calculate tpr and fpr.\n",
        "  \n",
        "  Output: Definite integral as approximated by trapezoidal rule.\n",
        "\n",
        "  '''\n",
        "  # variable initialization\n",
        "  s = data['y'].value_counts()\n",
        "  P = s[1]\n",
        "  N = s[0]\n",
        "  tpr = []\n",
        "  fpr = []\n",
        "\n",
        "  #calculating auc \n",
        "  for element in tqdm_notebook(data['proba']):\n",
        "    data['y_pred'] = predict(data, 'proba', element)\n",
        "    confusion_matrix= calculate_confusion(data, 'y', 'y_pred')\n",
        "    tpr.append(confusion_matrix['tp']/P)\n",
        "    fpr.append(confusion_matrix['fp']/N)\n",
        "    data.drop(columns=['y_pred'])\n",
        "  \n",
        "  return np.trapz(tpr, fpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgA1H81j_BsS"
      },
      "source": [
        "# sort the value by 'proba' value \n",
        "data_a = data_a.sort_values(by='proba',ascending=False)\n",
        "\n",
        "# droping 'y_pred' column\n",
        "data_a.drop(columns=['y_pred'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adDQSVfqAV8J"
      },
      "source": [
        "# calculate AUC score \n",
        "AUC_score=auc(data_a)\n",
        "\n",
        "# display auc score\n",
        "print(\"The AUC Score is :\",AUC_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5KZem1BQvn2"
      },
      "source": [
        "<pre>\n",
        "<font color='red'><b>B.</b></font> Compute performance metrics for the given data <strong>5_b.csv</strong>\n",
        "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
        "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
        "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
        "\n",
        "<pre>\n",
        "<ol>\n",
        "<li> Compute Confusion Matrix </li>\n",
        "<li> Compute F1 Score </li>\n",
        "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a></li>\n",
        "<li> Compute Accuracy Score </li>\n",
        "</ol>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2sKlq0YQvn5"
      },
      "source": [
        "# write your code\n",
        "\n",
        "# reading dataframe b\n",
        "data_b = pd.read_csv(\"/content/5_b.csv\")\n",
        "\n",
        "data_b.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA_iWVxdjwch"
      },
      "source": [
        "# thresh hold value intialization\n",
        "thresh_hold = 0.5\n",
        "\n",
        "# predicting y_pred using predict method\n",
        "data_b['y_pred'] = predict(data_b, 'proba', thresh_hold)\n",
        "\n",
        "# calculating confusion matrix\n",
        "confusion_matrix_b = calculate_confusion(data_b, 'y', 'y_pred')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z3b_2e1kNtY"
      },
      "source": [
        "# confusion matrix result display\n",
        "print(\"The confusion matrix is :\",confusion_matrix_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwPi7FcXkYP0"
      },
      "source": [
        "# F1 score calculation\n",
        "x = data_b.y.value_counts()\n",
        "P = x[1]\n",
        "\n",
        "# precision calculation = TP / (TP + FP)\n",
        "precision_b = confusion_matrix_b['tp']/(confusion_matrix_b['tp']+confusion_matrix_b['fp'])\n",
        "\n",
        "# recall calculation = Tp / P\n",
        "recall_b = confusion_matrix_b['tp'] / P\n",
        "\n",
        "#F1 calculation\n",
        "f1_b = 2*precision_b*recall_b / (precision_b + recall_b)\n",
        "\n",
        "print(\"The F1 Score is :\", f1_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IevUsvNUlF1t"
      },
      "source": [
        "# Accuracy calculation\n",
        "Acc_b = (confusion_matrix_b['tp'])+confusion_matrix_b['tn']/data_b.shape[0]\n",
        "print(\"The accuracy is :\", Acc_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS_wXn_ClZfF"
      },
      "source": [
        "# Auc score calculation\n",
        "data_b = data_b.sort_values(by='proba', ascending=False)\n",
        "print(\"The Accuracy is :\", Acc_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiPGonTzQvoB"
      },
      "source": [
        "<font color='red'><b>C.</b></font> Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data <strong>5_c.csv</strong>\n",
        "<br>\n",
        "\n",
        "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
        "\n",
        "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
        "\n",
        "<pre>\n",
        "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
        "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5HIJzq1QvoE"
      },
      "source": [
        " # write your code\n",
        "\n",
        " # reading dataframe c\n",
        " data_c = pd.read_csv(\"/content/5_c.csv\")\n",
        " data_c.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2klV05mI0Z3"
      },
      "source": [
        "# min metric calculation method\n",
        "\n",
        "def Min_MetricCalculation(data):\n",
        "  '''\n",
        "  Argument:\n",
        "  data: DataFrame\n",
        "\n",
        "  AIM:\n",
        "  Min matrix calculation\n",
        "\n",
        "  Output:\n",
        "  metric as dictionary\n",
        "  '''\n",
        "  # initialize variables\n",
        "  s = data['y'].value_counts()\n",
        "  P = s[1]\n",
        "  N = s[0]\n",
        "  tpr = []\n",
        "  fpr = []\n",
        "  metric = {}\n",
        "  for element in tqdm_notebook(data['prob']):\n",
        "    data['y_pred'] = predict(data, 'prob', element)\n",
        "    confusion_matrix_c = calculate_confusion(data, 'y', 'y_pred')\n",
        "    metric_value = (500*confusion_matrix_c['fn'])+(100*confusion_matrix_c['fp'])\n",
        "    metric[element] = metric_value\n",
        "    data.drop(columns=['y_pred'])\n",
        "  \n",
        "  return (metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8C-QqYBNGWF"
      },
      "source": [
        "# sorting dataframe values by prob column\n",
        "data_c = data_c.sort_values(by='prob', ascending=False)\n",
        "\n",
        "# calculating min matrix \n",
        "result = Min_MetricCalculation(data_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUvBTl1zNRSH"
      },
      "source": [
        "temp = min(result.values())\n",
        "res = [key for key in result if result[key] == temp]\n",
        "\n",
        "print(\"The key:value pair for min value of the specified metric is :\", res, temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD4CcgjXQvoL"
      },
      "source": [
        "<pre>\n",
        "<font color='red'><b>D.</b></font> Compute performance metrics(for regression) for the given data <strong>5_d.csv</strong>\n",
        "    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
        "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
        "<ol>\n",
        "<li> Compute Mean Square Error </li>\n",
        "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
        "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
        "</ol>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JuA0zUnQ5ox"
      },
      "source": [
        "# reading dataframe d\n",
        "data_d = pd.read_csv(\"/content/5_d.csv\")\n",
        "data_d.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2STAIj1Gxxbh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEMNoPmRQ9NL"
      },
      "source": [
        "# error calculation method\n",
        "def Error_Calculation(data, column1, column2):\n",
        "  value = []\n",
        "\n",
        "  for index, (value1, value2) in enumerate(zip(data[column1], data[column2])):\n",
        "    value.append(value1-value2)\n",
        "\n",
        "  return value\n",
        "\n",
        "# absolute error calculation method\n",
        "def absolute_error(data,column):\n",
        "    val=[]\n",
        "    for index,value in enumerate(data[column]):\n",
        "        val.append(abs(value))\n",
        "    return val\n",
        "\n",
        "# mean sqaure error calculation method\n",
        "def mean_sqaure_error(data, column):\n",
        "  return ss_res(data,column)/len(data[column])\n",
        "\n",
        "# method for mape\n",
        "def mape(data, column1, column2):\n",
        "  value = sum(data[column1]/sum(data[column2]))\n",
        "  return value\n",
        "\n",
        "# method for ss_res\n",
        "def ss_res(data, column):\n",
        "  value = 0\n",
        "  for index, value in enumerate(data[column]):\n",
        "    value = value + (value*value)\n",
        "  \n",
        "  return value\n",
        "\n",
        "# method for ss_tot\n",
        "def ss_tot(data, column):\n",
        "  value = 0\n",
        "  mean_value = data_d['y'].mean()\n",
        "  \n",
        "  for index, value in enumerate(data[column]):\n",
        "    value = value + (value-mean_value)*(value-mean_value)\n",
        "\n",
        "  return value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjXXO2vmTyCv"
      },
      "source": [
        "# calculating the error \n",
        "data_d['error'] = Error_Calculation(data_d, 'y', 'pred')\n",
        "\n",
        "# calculating the absolute error\n",
        "data_d['abs_error'] = absolute_error(data_d,'error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6nhXSvkUGgE"
      },
      "source": [
        "MSE = mean_sqaure_error(data_d, 'error')\n",
        "print(\"The Mean sqaured error is :\", MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74gvbGhaVaw9"
      },
      "source": [
        "# calculating mape \n",
        "MAPE = mape(data_d, 'abs_error', 'y')\n",
        "print(\"The MAPE value is :\", MAPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTZjf9hFVzg5"
      },
      "source": [
        "# calculating the co-efficient of determination with formula and methods\n",
        "SS_RES = ss_res(data_d, 'error')\n",
        "SS_TOT = ss_tot(data_d, 'y')\n",
        "R_square = 1 - (SS_RES/ SS_TOT)\n",
        "print(\"The Co-efficient of determination value is :\", R_square)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L5HsPPGWBiR"
      },
      "source": [
        "# Reference \n",
        "\n",
        "# https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62 - for different way to understand\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}